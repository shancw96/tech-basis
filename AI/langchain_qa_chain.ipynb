{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 记录如何使用高度集成的qa chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.callbacks.manager import (AsyncCallbackManager)\n",
        "from langchain.chains.chat_vector_db.prompts import (CONDENSE_QUESTION_PROMPT, QA_PROMPT)\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.vectorstores.base import VectorStore\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "OPENAI_API_KEY = \"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\"\n",
        "\n",
        "import os\n",
        "os.environ[\"http_proxy\"] = \"http://127.0.0.1:1087\"\n",
        "os.environ[\"https_proxy\"] = \"http://127.0.0.1:1087\"\n",
        "os.environ[\"all_proxy\"] = \"socks5://127.0.0.1:1080\"\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "创建vectorDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "persist_directory = 'blog_vector_storage'\n",
        "\n",
        "# 假设我们已经持久化了一个vectorstore\n",
        "vectorStore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "query = \"docker 如何创建一个mysql容器？\"\n",
        "docs = vectorStore.similarity_search(query)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "创建问题生成chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# llm = OpenAI(temperature=0.8, openai_api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo\")\n",
        "# streaming_llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0.8, openai_api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo\")\n",
        "# question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.8, openai_api_key=OPENAI_API_KEY)\n",
        "streaming_llm = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0.8, openai_api_key=OPENAI_API_KEY)\n",
        "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT, verbose=True)\n",
        "\n",
        "doc_chain = load_qa_chain(streaming_llm, chain_type=\"stuff\", prompt=QA_PROMPT, verbose=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Version1: ConversationalRetrievalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qa = ConversationalRetrievalChain(\n",
        "    retriever=vectorStore.as_retriever(), \n",
        "    combine_docs_chain=doc_chain, \n",
        "    question_generator=question_generator, \n",
        "    verbose=True\n",
        "    )\n",
        "result = qa({\"question\": \"python 如何proxy？\", \"chat_history\": []})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Version2: RetrievalQAWithSourcesChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain import OpenAI\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "llm = ChatOpenAI(temperature=0.8, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "chain = RetrievalQAWithSourcesChain.from_chain_type(streaming_llm, chain_type=\"map_reduce\", retriever=vectorStore.as_retriever())\n",
        "result = chain({\"question\": \"docker 如何运行 mysql mongodb\"})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
